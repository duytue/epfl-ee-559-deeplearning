{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Going Deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important issue to train deep architectures is to control the ampliture of the gradient, which is tightly related to controlling activations.\n",
    "In particular, we must ensure that:\n",
    "* the gradient does not vanish\n",
    "* gradient amplitude is homogeneous so that all parts of the netword train at the same rate\n",
    "* the gradient does not vary too unpredictably when the weights change\n",
    "\n",
    "An addional issue for training large architectures is the computational cost -> usually the main practical problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectifiers\n",
    "\n",
    "* ReLU: its derivative does not vanish :D, hence better that *tanh* function. And also its way faster.\n",
    "![relu_vs_tanh](img/lecture6/relu_vs_tanh.png)\n",
    "\n",
    "However, sometimes ReLU function may lead to some dead neurons (that will not activated). We come up with a new **Leaky ReLU**:\n",
    "![leaky_relu](img/lecture6/leaky_relu.png)\n",
    "\n",
    "\n",
    "* **Maxout layer**: takes max of several linear units. It can encode ReLU, absolute value, or approximate any convex function.\n",
    "\n",
    "* **Concatenated Rectified Linear Unit (CReLU)**: doubles the number of activations but keeps the norm of signal intact during forward and backward passes.\n",
    "![crelu](img/lecture6/crelu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "A regularization technique.\n",
    "Consists of removing units at random during the forward pass on each sample, and putting them all back during test.\n",
    "\n",
    "This method increases independence between units, and distribute the representation. Hence, generally improves performances.\n",
    "\n",
    "> In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing. Therefore, units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, dropout prevents co-adaptation by making the presence of other hidden units unreliable. Therefore, a hidden unit cannot rely on other specific units to correct its mistakes. It must perform well in a wide variety of different contexts provided by the other hidden units.\n",
    "(Srivastava et al., 2014)   \n",
    "\n",
    "![dropout](img/lecture6/dropout_network.png)\n",
    "\n",
    "In PyTorch: torch.nn.Dropout (torch.Module): default probability p = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n",
      "tensor([[ 0.,  0.,  4.,  0.,  0.,  0.,  0.,  4.,  0.],\n",
      "        [ 0.,  4.,  0.,  0.,  4.,  0.,  4.,  4.,  0.],\n",
      "        [ 4.,  0.,  0.,  0.,  4.,  0.,  0.,  0.,  4.]])\n",
      "tensor([[ 0.0000,  0.0000,  2.8284,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.8284,  0.0000],\n",
      "        [ 0.0000,  2.0000,  0.0000,  0.0000,  2.0000,  0.0000,  2.0000,\n",
      "          2.0000,  0.0000],\n",
      "        [ 2.3094,  0.0000,  0.0000,  0.0000,  2.3094,  0.0000,  0.0000,\n",
      "          0.0000,  2.3094]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(Tensor(3,9).fill_(1.0), requires_grad = True)\n",
    "print (x.data)\n",
    "\n",
    "dropout = nn.Dropout(p = 0.75)\n",
    "y = dropout(x)\n",
    "print (y.data)\n",
    "\n",
    "l = y.norm(2, 1).sum()\n",
    "l.backward()\n",
    "print (x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-6ef5b357fb62>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-6ef5b357fb62>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    (0): Linear(10 -> 100)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\" In a network, we can simply add dropout as a layer\"\"\"\n",
    ">>> model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Dropout(), nn.Linear(100, 2))\n",
    "\"\"\" A model using drop out has to be set in **train** or **test** mode\"\"\"\n",
    ">>> model = nn.Sequential(nn.Linear(10, 100), nn.Dropout(), nn.Linear(100, 2))\n",
    ">>> dropout.training\n",
    "True\n",
    ">>> model.train(False)\n",
    "Sequential (\n",
    "    (0): Linear(10 -> 100)\n",
    "    (1): Dropout(p = 0.5)\n",
    "    (2): Linear(100 -> 2)\n",
    ")\n",
    ">>> dropout.training\n",
    "False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a 2D activation maps, units are generally correlated, and dropout has virtually no effect.\n",
    "-> Spatial Dropout: drops channels instead of units.\n",
    "\n",
    "```python\n",
    ">>> dropout2d = nn . Dropout2d ()\n",
    ">>> x = Variable ( Tensor (2 , 3 , 2 , 2) . fill_ (1.0) )\n",
    ">>> dropout2d ( x )\n",
    "Variable containing :\n",
    "(0 ,0 ,. ,.) =\n",
    "0 0\n",
    "0 0\n",
    "(0 ,1 ,. ,.) =\n",
    "0 0\n",
    "0 0\n",
    "(0 ,2 ,. ,.) =\n",
    "2 2\n",
    "2 2\n",
    "(1 ,0 ,. ,.) =\n",
    "2 2\n",
    "2 2\n",
    "(1 ,1 ,. ,.) =\n",
    "0 0\n",
    "0 0\n",
    "(1 ,2 ,. ,.) =\n",
    "2 2\n",
    "2 2\n",
    "[ torch . FloatTensor of size 2 x3x2x2 ]\n",
    "```\n",
    "\n",
    "Another type of dropout is dropconnect: drop connection between units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation normalization\n",
    "\n",
    "To keep proper statistics of the activations and derivatives.\n",
    "Why?\n",
    "* To learn (much) faster. \n",
    "-> **Batch normalization**\n",
    "\n",
    "* During training, batch normalization **shifts and rescales according to the mean and variance estimated on the batch.**\n",
    "* During test, it simply shifts and rescales according to the empirical moments estimated during training.\n",
    "\n",
    "Further more, there is **layer normalization** proposed by Ba et al. (2016). It normalizes across layers instead of batch. This gives similar or better improvements than BN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Networks\n",
    "\n",
    "Residual: the difference between the value before the block and the one needed after\n",
    "\n",
    "\n",
    "![residual](img/lecture6/residual.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Initialization\n",
    "\n",
    "### Look Linear initialization\n",
    "Combine LL initialization with CReLU will make the network linear initially.\n",
    "\n",
    "![ll_init](img/lecture6/ll_init.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Techniques to enable training of deep networks:\n",
    "* **rectifiers** to prevent the gradient from vanishing during backward pass\n",
    "* **drop-out** to force a distributed representation(**prevent overfitting, dependence between units**)\n",
    "* **batch normalization** to dynamically maintain the statistics of activations (makes training much faster)\n",
    "* **identity pass-through (resnet, residual block)** to keep a structured gradient and distribute representation\n",
    "* **smart initialization** (look linear) to put the gradient in a good regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Using multiple GPUs \"\"\"\n",
    "\n",
    "class Dummy nn.Module) :\n",
    "    def __init__(self, m  :\n",
    "        super( Dummy,self).__init__()\n",
    "        self.m = m\n",
    "    def forward (self, x ) :\n",
    "        print (’Dummy.forward’ , x.size(), torch.cuda.current_device())\n",
    "        return self.m(x)\n",
    "x = Variable(Tensor(50, 10).normal_()))\n",
    "m = Dummy(nn.Linear(10, 5))\n",
    "x = x.cuda()\n",
    "m = m.cuda()\n",
    "\n",
    "print (’Without data_parallel’)\n",
    "y = m(x)\n",
    "print ()\n",
    "mp = nn.DataParallel(m)\n",
    "print (’With data_parallel’)\n",
    "y = mp(x)\n",
    "\n",
    "# Output\n",
    "Without data_parallel\n",
    "Dummy . forward torch . Size ([50 , 10]) 0\n",
    "With data_parallel\n",
    "Dummy . forward torch . Size ([25 , 10]) 0\n",
    "Dummy . forward torch . Size ([25 , 10]) 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

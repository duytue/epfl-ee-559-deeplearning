{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "train_input, train_target, test_input, test_target = \\\n",
    "    prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: N x 1 x 28 x 28   \n",
    "Conv1: N x 32 x 24 x 24   \n",
    "pool1: N x 32 x 8 x 8   \n",
    "Conv2: N x 64 x 4 x 4   \n",
    "pool2: N x 64 x 2 x 2   \n",
    "view: N x 1 x 256   \n",
    "fc1: N x 1 x 200   \n",
    "fc2: N x 1 x 10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hiddens):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hiddens)\n",
    "        self.fc2 = nn.Linear(nb_hiddens, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, nb_hiddens):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 24, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(24, 48, kernel_size=4)\n",
    "        self.conv3 = nn.Conv2d(48, 96, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(96*2*2, nb_hiddens)\n",
    "        self.fc2 = nn.Linear(nb_hiddens, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1*28*28\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        # 24*24*24 -> 24*12*12\n",
    "        \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=3, stride=3))\n",
    "        # 48*9*9 -> 48*3*3\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        # 96*2*2\n",
    "        \n",
    "        x = F.relu(self.fc1(x.view(-1, 96*2*2)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size):\n",
    "    criterion = nn.MSELoss()\n",
    "    eta = 1e-1\n",
    "\n",
    "    for e in range(0, 25):\n",
    "        sum_loss = 0\n",
    "        # We do this with mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            # forward pass\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            # compute loss\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.data\n",
    "            \n",
    "            # reset gradient variables\n",
    "            model.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        # print(e, sum_loss)\n",
    "    print('Final sum_loss: ', sum_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*compute_nb_errors*\n",
    "* To compute the number of prediction mistakes using a “winner-take-all” rule, that is the class with\n",
    "the largest output is the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input, target, mini_batch_size):\n",
    "    nb_errors = 0\n",
    "    \n",
    "    for i in range(0, input.size(0), mini_batch_size):\n",
    "        output = model(input.narrow(0, i, mini_batch_size))\n",
    "        _, predicted_index = output.data.max(1)\n",
    "        for j in range(0, mini_batch_size):\n",
    "            if target.data[i+j, predicted_index[j]] < 0:\n",
    "                nb_errors += 1\n",
    "                \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sum_loss:  tensor(0.7994)\n",
      "error: 9.40% 94 / 1000\n",
      "Final sum_loss:  tensor(0.7642)\n",
      "error: 8.80% 88 / 1000\n",
      "Final sum_loss:  tensor(0.7445)\n",
      "error: 11.20% 112 / 1000\n",
      "Final sum_loss:  tensor(0.8316)\n",
      "error: 11.10% 111 / 1000\n",
      "Final sum_loss:  tensor(0.7700)\n",
      "error: 10.10% 101 / 1000\n",
      "Final sum_loss:  tensor(0.7821)\n",
      "error: 11.60% 116 / 1000\n",
      "Final sum_loss:  tensor(0.7222)\n",
      "error: 10.00% 100 / 1000\n",
      "Final sum_loss:  tensor(0.7828)\n",
      "error: 10.60% 106 / 1000\n",
      "Final sum_loss:  tensor(0.7677)\n",
      "error: 9.70% 97 / 1000\n",
      "Final sum_loss:  tensor(0.6800)\n",
      "error: 8.60% 86 / 1000\n"
     ]
    }
   ],
   "source": [
    "# Driver\n",
    "train_input, train_target = Variable(train_input), Variable(train_target)\n",
    "test_input, test_target = Variable(test_input), Variable(test_target)\n",
    "mini_batch_size = 100\n",
    "\n",
    "#train_model(model, train_input, train_target, mini_batch_size)\n",
    "## ex2 ##\n",
    "for i in range(10):\n",
    "    # default nn with 200 hidden units\n",
    "    model = Net(200)\n",
    "    \n",
    "    train_model(model, train_input, train_target, mini_batch_size)\n",
    "    nb_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "    print ('error: {:.02f}% {:d} / {:d}'.format(100*nb_errors / test_input.size(0), nb_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sum_loss:  tensor(3.5999)\n",
      "10 nb_hiddens -> error: 84.40% 844 / 1000\n",
      "Final sum_loss:  tensor(0.8077)\n",
      "50 nb_hiddens -> error: 10.40% 104 / 1000\n",
      "Final sum_loss:  tensor(0.7144)\n",
      "200 nb_hiddens -> error: 9.30% 93 / 1000\n",
      "Final sum_loss:  tensor(0.7058)\n",
      "500 nb_hiddens -> error: 11.10% 111 / 1000\n",
      "Final sum_loss:  tensor(0.6850)\n",
      "1000 nb_hiddens -> error: 8.20% 82 / 1000\n"
     ]
    }
   ],
   "source": [
    "## ex3 ##\n",
    "for k in [10, 50, 200, 500, 1000]:\n",
    "    # model with k hidden units\n",
    "    model =  Net(k)\n",
    "    \n",
    "    train_model(model, train_input, train_target, mini_batch_size)\n",
    "    nb_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "    print ('{:d} nb_hiddens -> error: {:.02f}% {:d} / {:d}'.format(k, 100*nb_errors / test_input.size(0), nb_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sum_loss:  tensor(0.9636)\n",
      "error: 9.30% 93 / 1000\n"
     ]
    }
   ],
   "source": [
    "## ex4 ##\n",
    "model = Net2(200)\n",
    "train_model(model, train_input, train_target, mini_batch_size)\n",
    "nb_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "print ('error: {:.02f}% {:d} / {:d}'.format(100*nb_errors / test_input.size(0), nb_errors, test_input.size(0)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
